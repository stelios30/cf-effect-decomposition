{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Experiments Setup\n",
    "\n",
    "To run the LLM-based experiment, we must setup a local instance of the LLM. We will utilize the [Ollama project](https://github.com/ollama/ollama) to run an instance of the LLAMA 2 (7B) model.\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "There are few environment variables that are required to initialize the model. First, make a local copy of the environment file by running `cp .env.example .env`. Then, change the variable values to point to your local directories. The Ollama API is available on the local port 11434. You can also access the GUI on the local port 3000. We use [python-dotenv](https://pypi.org/project/python-dotenv/) library to load the defined environment variables into this notebook. Note that you will have to restart the kernel every time you update the `.env` file for the changes to be reflected.\n",
    "\n",
    "## Docker and Docker Compose\n",
    "\n",
    "We have provided a Docker Compose configuration for a more streamlined setup process. First, make sure you have [Docker installed](https://docs.docker.com/engine/install/). Then, from the root of this repository, it suffices to run `docker compose up`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The config values specified here are used throughout the codebase and should work with their defaults if you are running Ollama via the provide Docker configuration. Don't forget to update the `.env` path, if it differs from the default one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from ced.tools.llm import LLMConfig, LLMGateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(dotenv_path=\"./.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Test the Model\n",
    "\n",
    "We create the model using our custom prompt and configuration variables. Note that a first run might take some time, as the model weights shall be downloaded (~3.8 GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LLMConfig()\n",
    "gateway = LLMGateway(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gateway.pull()\n",
    "assert gateway.create(config=LLMConfig.modelfile())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = gateway.generate(prompt=\"obs: A1 respawn; A2 respawn;\")\n",
    "r2 = gateway.generate(prompt=\"obs: A1 (PINK GREEN); A2 (PINK YELLOW);\", context=r1.context)\n",
    "r3 = gateway.generate(prompt=\"obs: A1 has PINK; A2 has PINK;\", context=r2.context)\n",
    "print(r1.response, r2.response, r3.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
